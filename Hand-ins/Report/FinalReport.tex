\documentclass[11pt,a4paper, titlepage]{article}

\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{graphicx,wrapfig}
\usepackage{float}
\usepackage[export]{adjustbox}
\usepackage{array}
\usepackage{comment}
\usepackage{tabularx,longtable}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption}

\renewcommand{\baselinestretch}{1.5}

\usepackage[width=16.00cm, left=2.50cm, top=2.5cm, bottom=2.5cm]{geometry}

\title{ARPA: Autonomous Robotic Pointer Arm}
\author{Helgerud, Erlend \and Håland, André}

\begin{document}
	\setlength\parindent{0pt}
	\setlength\parskip{10pt}
	\headheight
	\setlength{15pt}{}
	\footskip
	\setlength{33pt}{}
	\setcounter{secnumdepth}{4}
	\maketitle
	
\begin{abstract}
This report is written as part of a project in the course DFMS3200 - Simulation and Modelling at the University College of South-Eastern Norway. The background for the project is the idea of automation in industrial production lines in order to minimize costs and increase productivity. The solution proposed is an autonomous 5DOF robotic manipulator that perform actions based on a live video feed. The main focus of the project is to serve as a introductory case-study in robotic manipulation and control, and therein the usage of selected tools. The project is split in two, virtual simulation and physical representation. For controlling both the virtual and physical representation, the project utilizes ROS Kinetic (Robotic Operating System) in Ubuntu 16.04 with the majority of code written in C++. The physical robot is powered by 12 AA batteries together with two Arduino MEGA2560's. The vision system is based in Python 2.6 and OpenCV2. In order to provide the virtual representation the project incorporates Gazebo together with needed models written in URDF/Xacro and configurations written in YAML. Kinematics are handled in Matlab's Robotics System Toolbox. As a final demonstration, the separate components of the project will be run with a 1:1 relationship between the virtual and physical representations, integrating all components of the project.
\end{abstract}
	
	\tableofcontents
	\newpage

\section{Introduction}
As a result of the high average of salaries in Norway a lot of the processing of food is outsourced to countries with a lower average salary \cite{berge2015}. An example is the outsourcing of processing of Norwegian salmon to Poland, which has a lower hourly rate of work by 13.5\% compared to Norway. A direct consequence of the difference is that companies like Marine Harvest export roughly processed salmon to Poland, and import fine processed salmon back to Norway \cite{digre2014}. To be able to preserve the processing of fish in Norway, there is a demand for a system that can remove defects such as blood and/or melanin defects from the surface of fish fillets.

Using the previous paragraph as the background, the motivation for this project is to conduct case-study in robotic manipulation and control in order to be familiarized with possible tools and technologies used in robotics.

The project itself has served as an introduction to robot technology, thus, a big part of the project has been the learning of the different technologies which will be discussed in detail later in the report.

\subsection{Contributions}
The project aims to contribute an environment consisting of a physical 5DOF robotic manipulator with a 1:1 relationship to a virtual simulation which can be used for rapid prototyping and testing. The system has incorporated a detection system to automate the process of finding markers. In addition, the delivered report and GitHub repository should serve as a contribution to student inheriting the project at a later stage.

The physical robot is powered by two Arduino Mega2560's and 12 AA batteries. One of the Arduinos functions as the controller for the physical robot, while the other one serves solely as a power supply to a servo motor.

\subsection{Tools}
\subsubsection{Hardware}
	\begin{center}           
    \begin{longtable}{| c | c | c | c | c |}
              \hline
%Utvid ved å kopiere: xx.xx.2018 & IDXX & Oppdatering \\  \hline            
\multicolumn{5}{|c|}{\textbf{Hardware}} \\ \hline \endhead
\textbf{Component} & \textbf{Quantity} & \textbf{Usage} & \textbf{Link} & \textbf{Ordliste} \\ \hline             
              \textbf{Prosjektplan} &  Pp & X & X & \\ \hline
              
              \textbf{Krav og Design} &  Kd & X & X &  \\ \hline
             
              \textbf{Test} & Te & X & & \\ \hline
              
              \textbf{Risiko} & Ri & X & X &\\ \hline
              
              \textbf{Konsept} & Kon & X & X & X \\ \hline
              
              \textbf{Research} & Re & X &  & X \\  \hline
              
              \textbf{Teknisk} & Tek & X & X & X \\  \hline
              
              \textbf{Vedlegg} & & & & \\  \hline
              
              \textbf{Bibliografi} & & & & \\  \hline
                                       
\end{longtable}
\end{center}      




\subsubsection{Software dependencies}
This section will summarize all the software dependencies used in the development of ARPA.
	\begin{itemize}
		\item \textbf{Ubuntu 16.04 LTS:} Operating System [http://releases.ubuntu.com/16.04.4/]
		\item \textbf{Robotic Operating System(ROS) Kinetic:} Framework for robot software [http://wiki.ros.org/kinetic/Installation/Ubuntu]
		\item \textbf{rosserial\_arduino:} Include Arduino into ROS node network [http://wiki.ros.org/rosserial\_arduino]
		\item \textbf{MATLAB:} Programming platform [https://se.mathworks.com/products/matlab.html]
		\item \textbf{Robotics System Toolbox:} MATLAB add-on providing different robotic software [https://se.mathworks.com/products/robotics.html]
		\item \textbf{OpenCV2:} Computer vision library [https://opencv.org/]
		\item \textbf{Python:} Programming language [https://www.python.org/]
	\end{itemize}
	
%\section{Related works}
\section{System description}
	

\subsection{Architecture}
The idea of ARPA is to have a 1:1 relationship between a physical robotic arm, and a simulated model in Gazebo. This means that the model in Gazebo should contain (as good as) the same measurements as the physical robot. They will also receive the same control signals leading to synchronized movement. Figure \ref{fig:architecture} show this relationship between the real world and the simulated world.
	
	\begin{figure}[H]
		\includegraphics[width=\linewidth]{../Diagrams/Architecture.png}
		\caption{System architecture}
		\label{fig:architecture}
	\end{figure}
	
	ARPA is divided into nodes. These nodes each has its own tasks and communicate with each other on different topics to exchange information. Figure \ref{fig:nodegraph} shows the nodes of ARPA, and the topics they communicate on. A circle represents a node, whilst a rectangle represents a topic. The controller node is the center of all actions. It handles the interface between the camera, MATLAB, the physical robotic arm and the model in gazebo. The camera node  sends a Cartesian point in space to the controller. The MATLAB node handles inverse kinematics. It receives Cartesian coordinates from the controller, and sends back five joint angles. The 5DOF Robot node controls the physical robotic arm by writing received angles to the servo motors. The Gazebo node listens to the same topics as 5DOF Robot, and will simulate a robotic model with a 1:1 relationship with the physical arm.
	
As described in \textbf{todo: refer to XX}, we had some challenges with finding the inverse kinematics and have to depend on the Robotics System Toolbox from MATLAB. When running MATLAB as a ROS node, we discovered that it ran quite slowly together with Gazebo. For this reason, we decided to run the ROS network over two computers to separate computing power. This is done by setting a couple of environmental variables in every terminal that uses ROS. Two shell scripts were made for this, and can be sourced to achieve the distributed ROS network. The different colored nodes in figure \ref{fig:nodegraph} represents which computer the node runs on.

	\begin{figure}[H]
		\includegraphics[width=\linewidth]{../Diagrams/NodeGraph-v1.png}
		\caption{Node graph}
		\label{fig:nodegraph}
	\end{figure}
	

The nodes previously described is made by ROS packages. Figure \ref{fig:dir-list} shows the packages used in ARPA. The following subsections will describe each package, including why and how they are implemented.

	\begin{figure}[H]
		\includegraphics[width=\linewidth]{../Diagrams/Packages.png}
		\caption{Packages}
		\label{fig:dir-list}
	\end{figure}
	
	
\subsubsection{Camera}
The camera package contains the python-script that is responsible for handling object detection in ARPA. By placing red markers in the physical coordinate system of ARPA, the user will be able to track the markers with the live video feed from OpenCV. The script will after calibration continuously map received coordinates from the OpenCV coordinate frame to coordinates corresponding to the physical coordinate frame on the rig. The calibration and usage during runtime will be elaborated in finer detail in \textbf{XX: SECTION}.

The color-detection is done by applying a red mask in the HSV-colorspace on each frame from the live video. By finding the contours of the image the marker can be detected and the script further finds the center of the detected point. The script is incorporated in the ROS environment as a node that publishes the mapped cartesian x and y coordinates of a desired point. The script can be extended to detect more colors by adding additional

\subsubsection{Controller}
The controller package handles the interface between the other nodes. First of all, it receives all the coordinates from the camera node, but will only publish them to MATLAB if the \textit{ready.sh} script has been run. Optionally, a message containing 1 (true) can be published on the\textit{/camera\_ready} topic. After the controller has published the coordinates to MATLAB, it will receive a response in the form of five joint angles. These joint angles are further sent to their respective topics. Immediately after the angles are published, the controller sleeps for five seconds. During these five seconds, the robotic arm will have time to go to the given coordinates. After sleeping, the controller will start a function that sets the arm to a upright position, hereby referred to as the start position of the arm. It does so by rotating the top servos the opposite direction of the bottom servos, before it rotates the bottom servos. This is done because the torque of the bottom servos are to big to directly set all joints to start position. After approximately using three seconds to set the arm to start position, the controller is ready to get a new ready signal from the user.

\subsubsection{Robotic arm}
The robotic arm package contains the code run on the Arduino.
	
	
	
	\subsection{Information flow}
	
	The flow of information in ARPA starts when the user sends a ready signal to the controller node. This signals that the controller will accept a Cartesian coordinate from the camera. This coordinate  further sent to MATLAB for processing. MATLAB returns five joints angles to the controller. These are the angles used to control both the physical arm and the Gazebo model. Hence, the controller publishes them to their respective topics, where they are picked up by both the Gazebo node and the 5DOF Robot node. After the control node has sent the joint angles, it waits for 5 seconds before it sends a new set of joint angles used to set the physical and simulated arm back to starting position. This process can be repeated by sending a new ready signal. Figure \ref{fig:seq-diagram} shows the flow of information in the form of a UML Sequence diagram.
	
	
	\begin{figure}[H]
		\includegraphics[width=\linewidth]{../Diagrams/SequenceDiagram-v1.png}
		\caption{UML Sequence diagram}
		\label{fig:seq-diagram}
	\end{figure}
	
	\subsection{Starting program}
	\label{startup}
	
	When running ARPA over two computers, one of them must be chosen to run as the ROS master. The selected master computer must source the master.sh script in every terminal used for running ROS commands. Next, the slave.sh script must be modified to contain the IP-address of the master computer. The address can be read from the output of the master.sh script. When the slave script is modified to contain the correct address, it must be sourced in all terminals on the slave computer that runs any ROS commands.
	
	Next up is to launch all ROS nodes. First of all, the setup.bash script in the devel directory of the catkin workspace must be sourced. When this is done on the master computer, the Arduino controlling the servos of the robotic arm must be connected to the computer. When it is connected, the command \textit{roslaunch robotic\_arm master.launch} can be run. Note that the launch file is configured to setup the ROS node on /dev/ttyACM0. This means that either the Arduino running the code must be connected before the Arduino used for power supply (see ), or the launch file must be modified to contain the correct device parameter. The roslaunch command starts the camera, controller and the 5DOF robot node. 
	
\subsubsection{Calibration of camera}
The detection system used in ARPA is based in Python and the OpenCV library, and can be used together with an internal or external camera. The python script is in the beginning made so that a user has to go through a calibration phase to ensure mapping from the OpenCV coordinate frame to the coordinate frame of the physical rig.

Before running \textit{roslaunch robotic\_arm master.launch} there should be a red marker placed in ARPA's origo. The x-axis on the physical rig should also be parallel with the bottom border of the OpenCV window frame. The camera will during the first five seconds self-calibrate for lighting and focus, before masking the picture to find the red marker representing origo. After capturing 50 frames there is a five second pause in which the user will have to move the marker from origo to x-max, after an additional 50 frames the same will be repeated for y. When the script has defined all needed points of the coordinate frame, the values from the OpenCV frame is mapped to the lengths of the physical coordinate frame defined in the script. This method ensures that the same calibration method can be used independent of the cameras height above the rig, as long as origo, x-max and y-max is in the camera-frame.
	
	As with the master computer, the slave must also source the setup.bash script in the devel directory of the catkin workspace. When this is done, the command \textit{roslaunch simple\_robotic\_model joints.launch} can be run. This starts both the Gazebo and MATLAB node.
	
	\subsection{User interface}

	The user interface of ARPA is split into two: one on the master computer and one on the slave computer. Figure \ref{fig:ui-master} shows the user interface of the master. The window to the left is a live feed of the camera. It shows the user the detection of a red dot, i.e. where ARPA will move when given a ready signal. The window to the right allows the user to send this ready signal. When the ready.sh script is executed, the controller node will accept the next incoming coordinates and execute all necessary communications and actions for both the physical arm and the Gazebo model to move.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.95\linewidth]{../Diagrams/UI-master.png}
		\caption{User interface: Master}
		\label{fig:ui-master}
	\end{figure}
	
	Figure \ref{fig:ui-slave} shows the user interface of the slave. Gazebo is launched as described in \textit{\ref{startup} Starting program}, but the rqt window on the left is not included as a part of the launch file. The user can choose to start rqt from a new terming through the \textit{rqt} command. The preferred setup for this project was to have rqt run the console plugin to show all the ROS messages in a GUI.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.95\linewidth]{../Diagrams/UI-slave.png}
		\caption{User interface: Slave}
		\label{fig:ui-slave}
	\end{figure}
	
	\subsection{Hardware}
	During the project we had to make some adjustments to parts of the hardware related to the robotic arm. This section will go explain which adjustments has been made, and why.
	\subsubsection{Rig}
	When we were handed the robotic arm at the beginning of the course, we did some testing of it. First of all, we discovered that it had both had some loose screws, and also missing some. This lead us to spending some time on making the arm more rigid and secure. Figure \ref{fig:early-rig} shows this stage of the project.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\linewidth]{../Diagrams/early-setup.jpg}
		\caption{Early stage of the hardware}
		\label{fig:early-rig}
	\end{figure}
	
	During the first tests of the robotic arm, we discovered that the torque would make the robot fall over. We decided to make a more stable mechanical rig, by attaching the robotic arm to a stable wooden rig. Figure \ref{fig:plank-rig} shows the process of making the wooden rig.
	
	\begin{figure}
	\centering
	
	
	\begin{subfigure}[t]{0.2\textwidth}
		
		\includegraphics[height=1.2in]{../Diagrams/plank1.jpg}
		
		
	\end{subfigure}%
	
	\begin{subfigure}[t]{0.2\textwidth}
		\centering
		\includegraphics[height=1.2in]{../Diagrams/plank2.jpg}
		
	\end{subfigure}%
	
	\begin{subfigure}[t]{0.2\textwidth}
		\includegraphics[height=1.2in]{../Diagrams/plank3.jpg}
		
	\end{subfigure}
	
	\begin{subfigure}[t]{0.2\textwidth}
		\includegraphics[height=1.2in]{../Diagrams/plank4.jpg}
		
	\end{subfigure}
	\caption{Process of making a wooden rig}
	\label{fig:plank-rig}	
	\end{figure}
	
	\subsubsection{Power supply}
	The servo motors are running nominally at 6V. As the arm consists of five motors, we could not power them all from the Arduino. We decided to buy battery holders to run 4 AA batteries in series. Since an AA battery is 1.5V, running four of them in series adds up to 6V.
	
	Due to the bigger torque on the bottom motors, we decided to supply the bottom three motors with 6V each,  having them run with nominal voltage level. The upper two motors of the arm are powered by one Arduino each, meaning they get 5V.
	
	
	
	%\section{Simulation and experiments}
	
	%\section{Discussion}
	
	%\section{Appendix}

\begin{thebibliography}{1}

\bibitem{berge2015} Aslak Berge. Avhengig av polske kniver. \textit{http://ilaks.no/avhengig-av-polske-kniver-2/}. Aug 2018.

\bibitem{digre2014} Hanne Digre. Lønnsom foredling av sjømat i Norge. 2014.

\end{thebibliography}	
	
\end{document}